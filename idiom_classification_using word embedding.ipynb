{"cells":[{"cell_type":"markdown","id":"32c089f4-b407-41fd-aeaf-e39473ad4883","metadata":{"id":"32c089f4-b407-41fd-aeaf-e39473ad4883"},"source":["# ü¶Ö**Go to NN**"]},{"cell_type":"code","execution_count":null,"id":"1d31db0b-0d65-4036-b80a-baee4d5eb4cd","metadata":{"colab":{"background_save":true},"id":"1d31db0b-0d65-4036-b80a-baee4d5eb4cd"},"outputs":[],"source":["# Input essential libraries\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import keras\n","from keras import optimizers\n","from keras import backend as K\n","from keras import regularizers\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Activation, Dropout, Flatten\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n","from keras.utils import plot_model\n","from keras.preprocessing import sequence\n","from keras.callbacks import EarlyStopping\n","\n","from tqdm import tqdm\n","from nltk.corpus import stopwords\n","from nltk.tokenize import RegexpTokenizer\n","import os, re, csv, math, codecs\n","\n","sns.set_style(\"whitegrid\")\n","np.random.seed(50)\n","\n","MAX_NB_WORDS = 10000\n"]},{"cell_type":"code","execution_count":null,"id":"knv0YKQqOMUg","metadata":{"id":"knv0YKQqOMUg"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"2b3ad2b6-c2b0-4bfa-b201-2aace017746a","metadata":{"colab":{"background_save":true},"id":"2b3ad2b6-c2b0-4bfa-b201-2aace017746a"},"outputs":[],"source":["# Loading the pre-trained Word2Vec model\n","import gensim\n","from gensim.models import FastText\n","from gensim.models import KeyedVectors\n","from gensim.models.fasttext import load_facebook_vectors"]},{"cell_type":"code","execution_count":null,"id":"SRaS-OQwoTcA","metadata":{"id":"SRaS-OQwoTcA"},"outputs":[],"source":["lm = load_facebook_vectors('/content/drive/MyDrive/cc.am.300.bin.gz')"]},{"cell_type":"code","execution_count":null,"id":"b5004edb-6665-42d6-96aa-b2ea86ef32e0","metadata":{"id":"b5004edb-6665-42d6-96aa-b2ea86ef32e0"},"outputs":[],"source":["print('found %s word vectors' % len(lm))"]},{"cell_type":"code","execution_count":null,"id":"8d7e6ac3-1389-4c01-b444-b710c8fc560e","metadata":{"id":"8d7e6ac3-1389-4c01-b444-b710c8fc560e"},"outputs":[],"source":["#load data\n","train_df = pd.read_csv('/content/drive/MyDrive/New_Approach/train_90.csv', sep=',', header=0)\n","test_df = pd.read_csv('/content/drive/MyDrive/New_Approach/test_10.csv', sep=',', header=0)\n","test_df = test_df.fillna('_NA_')\n","\n","print(\"train set: \", train_df.shape[0])\n","print(\"test set: \", test_df.shape[0])\n","\n","label_names = [\"label\"]\n","y_train = train_df[label_names].values\n","y_test = test_df[label_names].values"]},{"cell_type":"code","execution_count":null,"id":"3a1ef40e-1691-4485-b22d-3ca1e35b153c","metadata":{"id":"3a1ef40e-1691-4485-b22d-3ca1e35b153c"},"outputs":[],"source":["#visualize word distribution\n","train_df['doc_len'] = train_df['text'].apply(lambda words: len(words.split(\" \")))\n","max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n","#max_seq_len = 5\n","sns.histplot(train_df['doc_len'],  kde=True, color='b', label='doc_len')\n","plt.axvline(x=max_seq_len, color='k', linestyle='--', label='max_len')\n","plt.title('text length'); plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"407760a8-1760-417a-8889-f8449c12ec46","metadata":{"id":"407760a8-1760-417a-8889-f8449c12ec46"},"outputs":[],"source":["#visualize word distribution\n","test_df['doc_len'] = test_df['text'].apply(lambda words: len(words.split(\" \")))\n","max_seq_len = np.round(test_df['doc_len'].mean() + test_df['doc_len'].std()).astype(int)\n","#max_seq_len = 5\n","sns.histplot(test_df['doc_len'], kde=True, color='b', label='doc_len')\n","plt.axvline(x=max_seq_len, color='k', linestyle='--', label='max_len')\n","plt.title('text length'); plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"d5de824b-551f-4dc9-b1b5-ff54ab5d9315","metadata":{"id":"d5de824b-551f-4dc9-b1b5-ff54ab5d9315"},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","tokenizer = RegexpTokenizer(r'\\w+')\n","\n","raw_docs_train = train_df['text'].tolist()\n","raw_docs_test = test_df['text'].tolist()\n","num_classes = len(label_names)\n","\n","processed_docs_train = []\n","for doc in tqdm(raw_docs_train):\n","    tokens = tokenizer.tokenize(doc)\n","    #filtered = [word for word in tokens if word not in stop_words]\n","    processed_docs_train.append(\" \".join(tokens))\n","#end for\n","\n","processed_docs_test = []\n","for doc in tqdm(raw_docs_test):\n","    tokens = tokenizer.tokenize(doc)\n","    #filtered = [word for word in tokens if word not in stop_words]\n","    processed_docs_test.append(\" \".join(tokens))\n","#end for\n","\n","print(\"tokenizing input data...\")\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n","tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)\n","word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n","word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n","word_index = tokenizer.word_index\n","print(\"dictionary size: \", len(word_index))\n","\n","#pad sequences\n","word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len, padding='post')\n","word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len, padding='post')"]},{"cell_type":"code","execution_count":null,"id":"db400cf8-ca42-4c6c-9a0d-2df78b336cc4","metadata":{"id":"db400cf8-ca42-4c6c-9a0d-2df78b336cc4"},"outputs":[],"source":["#training params\n","batch_size = 64\n","num_epochs = 20\n","\n","#model parameters\n","num_filters = 64\n","embed_dim = 300\n","weight_decay = 1e-4"]},{"cell_type":"code","execution_count":null,"id":"620cf4f0-716d-472a-bfcf-75498a4da310","metadata":{"collapsed":true,"id":"620cf4f0-716d-472a-bfcf-75498a4da310"},"outputs":[],"source":["#embedding matrix\n","words_not_found = []\n","nb_words = min(MAX_NB_WORDS, len(word_index))\n","embedding_matrix = np.zeros((nb_words, embed_dim))\n","for word, i in word_index.items():\n","    if i \u003e= nb_words:\n","        continue\n","    embedding_vector = lm.get_vector(word)\n","    if (embedding_vector is not None) and len(embedding_vector) \u003e 0:\n","        # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector\n","    else:\n","        words_not_found.append(word)\n","print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"]},{"cell_type":"markdown","id":"4f169cd6-9b93-4dcb-8998-91c25beb1660","metadata":{"id":"4f169cd6-9b93-4dcb-8998-91c25beb1660"},"source":["## ‚úî **CNN**"]},{"cell_type":"code","execution_count":null,"id":"YJkBcaVjo8wi","metadata":{"id":"YJkBcaVjo8wi"},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout\n","from keras import optimizers, regularizers\n","\n","model_1 = Sequential()\n","model_1.add(Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=max_seq_len, trainable=True))\n","model_1.add(Conv1D(32, 5, activation='relu', padding='same'))\n","model_1.add(MaxPooling1D(2))\n","model_1.add(Conv1D(16, 5, activation='relu', padding='same'))\n","model_1.add(GlobalMaxPooling1D())\n","model_1.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.02)))\n","model_1.add(Dropout(0.3))\n","model_1.add(Dense(1, activation='sigmoid'))\n","optimizer = optimizers.Adam(learning_rate=0.0001)\n","model_1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"id":"ZZHHdEDssfbo","metadata":{"id":"ZZHHdEDssfbo"},"outputs":[],"source":["#define callbacks\n","early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n","callbacks_list = [early_stopping]"]},{"cell_type":"code","execution_count":null,"id":"p_y22s5usgx8","metadata":{"collapsed":true,"id":"p_y22s5usgx8"},"outputs":[],"source":["#model training\n","hist1 = model_1.fit(word_seq_train, y_train, batch_size=batch_size,\n","                    epochs=20, validation_split=0.1,\n","                    verbose=1, callbacks=callbacks_list)"]},{"cell_type":"markdown","id":"O0HPqAzlsKvW","metadata":{"id":"O0HPqAzlsKvW"},"source":["little bit fine tuned"]},{"cell_type":"code","execution_count":null,"id":"5782fb86-1deb-4b01-af4f-4bdde58ce162","metadata":{"id":"5782fb86-1deb-4b01-af4f-4bdde58ce162"},"outputs":[],"source":["#generate plots\n","plt.figure()\n","plt.plot(hist1.history['loss'], lw=2.0, color='b', label='train_loss')\n","plt.plot(hist1.history['val_loss'], lw=2.0, color='r', label='val_loss')\n","plt.title('Amharic Idiom Recognition')\n","plt.xlabel('Epochs')\n","plt.ylabel('Cross-Entropy Loss')\n","plt.legend(loc='upper right')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"0Cu5srJVo_nb","metadata":{"id":"0Cu5srJVo_nb"},"outputs":[],"source":["plt.figure()\n","plt.plot(hist1.history['accuracy'], lw=2.0, color='b', label='train_accuracy')\n","plt.plot(hist1.history['val_accuracy'], lw=2.0, color='r', label='val_accuracy')\n","plt.title('Amharic Idiom Recognition ')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend(loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","id":"05fbdab2-31c3-4bc6-989b-3eaeee2a2621","metadata":{"id":"05fbdab2-31c3-4bc6-989b-3eaeee2a2621"},"source":["## ‚úî **LSTM**"]},{"cell_type":"code","execution_count":null,"id":"nJ3Q83eezVYI","metadata":{"id":"nJ3Q83eezVYI"},"outputs":[],"source":["model_2 = Sequential()\n","model_2.add(Embedding(nb_words, embed_dim, weights=[embedding_matrix], trainable=False))\n","model_2.add(LSTM(64,dropout=0.4,recurrent_dropout=0.4, return_sequences=True))\n","model_2.add(Dropout(0.3))\n","model_2.add(LSTM(32,dropout=0.3,recurrent_dropout=0.3))\n","model_2.add(Dropout(0.3))\n","model_2.add(Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.02)))\n","model_2.add(Dropout(0.4))\n","model_2.add(Dense(1, activation='sigmoid'))\n","model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"id":"trUhD2yZTMxN","metadata":{"collapsed":true,"id":"trUhD2yZTMxN"},"outputs":[],"source":["#model training\n","hist2 = model_2.fit(word_seq_train, y_train, batch_size=batch_size, epochs=20,validation_split=0.1, verbose=1, callbacks=callbacks_list)"]},{"cell_type":"code","execution_count":null,"id":"ohstzSLCTW8T","metadata":{"collapsed":true,"id":"ohstzSLCTW8T"},"outputs":[],"source":["#generate plots\n","plt.figure()\n","plt.plot(hist2.history['loss'], lw=2.0, color='b', label='train_loss')\n","plt.plot(hist2.history['val_loss'], lw=2.0, color='r', label='val_loss')\n","plt.title('Amharic Idiom Recognition')\n","plt.xlabel('Epochs')\n","plt.ylabel('Cross-Entropy Loss')\n","plt.legend(loc='upper right')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"fs4t1vm39XAa","metadata":{"collapsed":true,"id":"fs4t1vm39XAa"},"outputs":[],"source":["plt.figure()\n","plt.plot(hist2.history['accuracy'], lw=2.0, color='b', label='train_accuracy')\n","plt.plot(hist2.history['val_accuracy'], lw=2.0, color='r', label='val_accuracy')\n","plt.title('Amharic Idiom Recognition ')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend(loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","id":"d46ab4c9-e85d-473a-b4af-117f4a7d5fd6","metadata":{"id":"d46ab4c9-e85d-473a-b4af-117f4a7d5fd6"},"source":["## ‚úî **Bi-LSTM**"]},{"cell_type":"code","execution_count":null,"id":"RqxUkBKRmbvU","metadata":{"id":"RqxUkBKRmbvU"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n","\n","model_3 = Sequential()\n","model_3.add(Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=max_seq_len, trainable=True))\n","model_3.add(Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.3)))\n","model_3.add(Dropout(0.4))\n","model_3.add(Bidirectional(LSTM(32, recurrent_dropout=0.3)))\n","model_3.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.02)))\n","model_3.add(Dropout(0.3))\n","model_3.add(Dense(1, activation='sigmoid'))\n","model_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"id":"RFdc_8vTDxyG","metadata":{"collapsed":true,"id":"RFdc_8vTDxyG"},"outputs":[],"source":["#model training\n","hist3 = model_3.fit(word_seq_train, y_train, batch_size=batch_size, epochs=20,validation_split=0.1, verbose=1, callbacks=callbacks_list)"]},{"cell_type":"code","execution_count":null,"id":"3FVZ-oeRD0nC","metadata":{"collapsed":true,"id":"3FVZ-oeRD0nC"},"outputs":[],"source":["plt.figure()\n","plt.plot(hist3.history['loss'], lw=2.0, color='b', label='train_loss')\n","plt.plot(hist3.history['val_loss'], lw=2.0, color='r', label='val_loss')\n","plt.title('Amharic Idiom Recognition')\n","plt.xlabel('Epochs')\n","plt.ylabel('Cross-Entropy Loss')\n","plt.legend(loc='upper right')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"iKO6ZgyFEG9_","metadata":{"collapsed":true,"id":"iKO6ZgyFEG9_"},"outputs":[],"source":["plt.figure()\n","plt.plot(hist3.history['accuracy'], lw=2.0, color='b', label='train_accuracy')\n","plt.plot(hist3.history['val_accuracy'], lw=2.0, color='r', label='val_accuracy')\n","plt.title('Amharic Idiom Recognition ')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend(loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","id":"b4453dd6-f408-4c4d-867b-d17118bfe500","metadata":{"id":"b4453dd6-f408-4c4d-867b-d17118bfe500"},"source":["## ‚úî **GRU**"]},{"cell_type":"code","execution_count":null,"id":"y-xJj0LyhyZr","metadata":{"id":"y-xJj0LyhyZr"},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Embedding, GRU, Dense, Dropout\n","from keras import optimizers\n","from keras.callbacks import EarlyStopping\n","\n","model_4 = Sequential()\n","model_4.add(Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=max_seq_len, trainable=True))\n","model_4.add(GRU(32, return_sequences=True, recurrent_dropout=0.4))\n","model_4.add(Dropout(0.5))\n","model_4.add(GRU(16, recurrent_dropout=0.4))\n","model_4.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.03)))\n","model_4.add(Dropout(0.4))\n","model_4.add(Dense(1, activation='sigmoid'))\n","optimizer = optimizers.Adam(learning_rate=0.0001)\n","model_4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n"]},{"cell_type":"code","execution_count":null,"id":"nWo-ThESLsFk","metadata":{"id":"nWo-ThESLsFk"},"outputs":[],"source":["\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"]},{"cell_type":"code","execution_count":null,"id":"2lpRSr3uiRAr","metadata":{"id":"2lpRSr3uiRAr"},"outputs":[],"source":["#model training\n","hist4 = model_4.fit(word_seq_train, y_train, batch_size=batch_size, epochs=20, validation_split=0.1, verbose=1) #, callbacks=callbacks_list)"]},{"cell_type":"code","execution_count":null,"id":"N10euGBECI7W","metadata":{"id":"N10euGBECI7W"},"outputs":[],"source":["plt.figure()\n","plt.plot(hist4.history['loss'], lw=2.0, color='b', label='train_loss')\n","plt.plot(hist4.history['val_loss'], lw=2.0, color='r', label='val_loss')\n","plt.title('Idiom Classifcation')\n","plt.xlabel('Epochs')\n","plt.ylabel('Cross-Entropy Loss')\n","plt.legend(loc='upper right')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"FsJV0B7ZJcXk","metadata":{"id":"FsJV0B7ZJcXk"},"outputs":[],"source":["plt.figure()\n","plt.plot(hist4.history['accuracy'], lw=2.0, color='b', label='train_accuracy')\n","plt.plot(hist4.history['val_accuracy'], lw=2.0, color='r', label='val_accuracy')\n","plt.title('Idiom Classification ')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend(loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","id":"jpnO8rbRpLrf","metadata":{"id":"jpnO8rbRpLrf"},"source":["## ***üìäEvaluation Result***"]},{"cell_type":"code","execution_count":null,"id":"LaOalYcypLIw","metadata":{"id":"LaOalYcypLIw"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","\n","# Define the vocabulary size\n","vocab_size = 8613  # Replace with the actual vocabulary size used in your model\n","\n","# Check for out-of-bounds indices in word_seq_test and replace them with a special token\n","out_of_bounds_indices = np.where(word_seq_test \u003e= vocab_size)\n","if out_of_bounds_indices[0].size \u003e 0:\n","  word_seq_test[out_of_bounds_indices] = vocab_size - 1  # Replace with a special token (e.g., \u003cUNK\u003e)\n","\n","# Convert word_seq_test to a TensorFlow tensor with a defined shape\n","word_seq_test_tensor = tf.constant(word_seq_test)\n","\n","# Now evaluate the model\n","model_1.evaluate(word_seq_test_tensor, y_test)"]},{"cell_type":"code","execution_count":null,"id":"OhqnMub2p092","metadata":{"id":"OhqnMub2p092"},"outputs":[],"source":["y_pred = np.where(model_1.predict(word_seq_test)\u003e.5,1,0)"]},{"cell_type":"code","execution_count":null,"id":"pqnIwRjOp0ud","metadata":{"id":"pqnIwRjOp0ud"},"outputs":[],"source":["y_pred = y_pred.ravel()\n","y_pred"]},{"cell_type":"code","execution_count":null,"id":"RObzJxfGqGu2","metadata":{"id":"RObzJxfGqGu2"},"outputs":[],"source":["y_test = y_test.ravel()\n","y_test"]},{"cell_type":"code","execution_count":null,"id":"rzXzk9YbqGmJ","metadata":{"id":"rzXzk9YbqGmJ"},"outputs":[],"source":["from sklearn import metrics\n","print(metrics.classification_report(y_pred, y_test))"]},{"cell_type":"code","execution_count":null,"id":"Gq57oPXkp0Y9","metadata":{"id":"Gq57oPXkp0Y9"},"outputs":[],"source":["from sklearn import metrics\n","import tensorflow as tf\n","y_pred = tf.cast(y_pred, tf.float32)\n","print(\"model Loss:\",tf.keras.losses.binary_crossentropy(y_test, y_pred))\n","print(\"model Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n","print(\"model Precision:\",metrics.precision_score(y_test, y_pred))\n","print(\"model Recall:\",metrics.recall_score(y_test, y_pred))\n","print(\"model F1-score:\",metrics.f1_score(y_test, y_pred))"]},{"cell_type":"code","execution_count":null,"id":"ieoUNeVAqddb","metadata":{"id":"ieoUNeVAqddb"},"outputs":[],"source":["from sklearn.metrics import roc_curve, auc\n","y_pred_proba = model_1.predict(word_seq_test)\n","fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n","roc_auc = auc(fpr, tpr)"]},{"cell_type":"code","execution_count":null,"id":"fdWJH1AIQW2v","metadata":{"id":"fdWJH1AIQW2v"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, auc\n","\n","# Calculate fpr, tpr, and thresholds\n","fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n","\n","# Calculate AUC\n","roc_auc = auc(fpr, tpr)\n","\n","# Plot ROC curve\n","plt.figure()\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n","\n","# Print AUC\n","print(\"Area Under the Curve (AUC):\", roc_auc)"]},{"cell_type":"code","execution_count":null,"id":"_ihhUJ5ilrkL","metadata":{"id":"_ihhUJ5ilrkL"},"outputs":[],"source":["# Evaluate CNN\n","loss_cnn, accuracy_cnn = model_1.evaluate(word_seq_test, tf.expand_dims(y_test, axis=-1)) # Add an extra dimension to y_test\n","print(\"CNN - Loss: {}, Accuracy: {}\".format(loss_cnn, accuracy_cnn))\n","\n","# Evaluate LSTM\n","loss_lstm, accuracy_lstm = model_2.evaluate(word_seq_test, tf.expand_dims(y_test, axis=-1)) # Add an extra dimension to y_test\n","print(\"LSTM - Loss: {}, Accuracy: {}\".format(loss_lstm, accuracy_lstm))\n","\n","# Evaluate Bi-LSTM\n","loss_bilstm, accuracy_bilstm = model_3.evaluate(word_seq_test, tf.expand_dims(y_test, axis=-1)) # Add an extra dimension to y_test\n","print(\"Bi-LSTM - Loss: {}, Accuracy: {}\".format(loss_bilstm, accuracy_bilstm))\n","\n","# Evaluate GRU\n","loss_gru, accuracy_gru = model_4.evaluate(word_seq_test, tf.expand_dims(y_test, axis=-1)) # Add an extra dimension to y_test\n","print(\"GRU - Loss: {}, Accuracy: {}\".format(loss_gru, accuracy_gru))"]},{"cell_type":"code","execution_count":null,"id":"soeR2VXIoieD","metadata":{"id":"soeR2VXIoieD"},"outputs":[],"source":["results = pd.DataFrame({\n","    'Model': ['CNN', 'LSTM', 'Bi-LSTM', 'GRU'],\n","    'Loss': [loss_cnn, loss_lstm, loss_bilstm, loss_gru],\n","    'Accuracy': [accuracy_cnn, accuracy_lstm, accuracy_bilstm, accuracy_gru]\n","})\n","\n","print(results)"]},{"cell_type":"code","execution_count":null,"id":"CoRFvPrcuVly","metadata":{"id":"CoRFvPrcuVly"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Bar plot for accuracy\n","plt.bar(results['Model'], results['Accuracy'])\n","plt.xlabel('Model')\n","plt.ylabel('Accuracy')\n","plt.title('Accuracy Comparison of Models')\n","plt.show()\n","\n","# Bar plot for loss\n","plt.bar(results['Model'], results['Loss'])\n","plt.xlabel('Model')\n","plt.ylabel('Loss')\n","plt.title('Loss Comparison of Models')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"DgziwXkKugxd","metadata":{"id":"DgziwXkKugxd"},"outputs":[],"source":["import numpy as np\n","\n","# Set width of bars\n","barWidth = 0.25\n","\n","# Set position of bar on X axis\n","r1 = np.arange(len(results['Accuracy']))\n","r2 = [x + barWidth for x in r1]\n","\n","# Make the plot\n","plt.bar(r1, results['Accuracy'], color='blue', width=barWidth, edgecolor='white', label='Accuracy')\n","plt.bar(r2, results['Loss'], color='red', width=barWidth, edgecolor='white', label='Loss')\n","\n","# Add xticks on the middle of the group bars\n","plt.xlabel('Model', fontweight='bold')\n","plt.xticks([r + barWidth for r in range(len(results['Accuracy']))], results['Model'])\n","\n","# Create legend \u0026 Show graphic\n","plt.legend()\n","plt.title('Accuracy and Loss Comparison of Models')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"9oNSyTZ8SJq0","metadata":{"id":"9oNSyTZ8SJq0"},"outputs":[],"source":["y_pred = np.where(model_1.predict(word_seq_test) \u003e 0.5, 1, 0)\n","y_pred = y_pred.ravel()  # Flatten the predictions"]},{"cell_type":"code","execution_count":null,"id":"tzvylefrSEpi","metadata":{"id":"tzvylefrSEpi"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","\n","cm = confusion_matrix(y_test, y_pred)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n","disp.plot()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"t_jospEIIPdl","metadata":{"id":"t_jospEIIPdl"},"outputs":[],"source":["new_text = [\"·àä·çç·âµ ·àç·àµ·å•·àÖ\"]\n","new_text_tokens = tokenizer.texts_to_sequences(new_text)\n","new_text_padded = sequence.pad_sequences(new_text_tokens, maxlen=max_seq_len, padding='post')"]},{"cell_type":"code","execution_count":null,"id":"mfSI6HhiIgXg","metadata":{"id":"mfSI6HhiIgXg"},"outputs":[],"source":["predictions = model_1.predict(new_text_padded)\n","print(predictions)"]},{"cell_type":"code","execution_count":null,"id":"tpQnHzgYIqFZ","metadata":{"id":"tpQnHzgYIqFZ"},"outputs":[],"source":["predicted_labels = (predictions \u003e 0.5).astype(int)\n","print(predicted_labels)"]}],"metadata":{"colab":{"name":"","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":5}